{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing various packages\n",
    "from random import random, seed\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from matplotlib import cm\n",
    "from matplotlib.ticker import LinearLocator, FormatStrFormatter\n",
    "import sys\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error as mse\n",
    "import time\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import random\n",
    "import seaborn as sns\n",
    "from activation_functions import *\n",
    "from optimizer_functions import *\n",
    "from cost_functions import *\n",
    "from Layer import Layer\n",
    "from NeuralNetwork import NeuralNetwork\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.layers import Input\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras import optimizers             \n",
    "from tensorflow.keras import regularizers           \n",
    "from tensorflow.keras.utils import to_categorical   \n",
    "from sklearn.neural_network import MLPRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 1000\n",
    "x = np.random.rand(n,1)\n",
    "y = 1+3*x+2*x**2+0.1*np.random.randn(n,1)\n",
    "\n",
    "x_lins = np.linspace(0,1,100)\n",
    "\n",
    "X = np.c_[np.ones((n,1)), x, x**2]\n",
    "\n",
    "Xtrain, Xtest, ytrain, ytest = train_test_split(X, y, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# grid search number of neurons and eta:\n",
    "\n",
    "n_neurons_list = [10,20,30,50,75]\n",
    "eta_list = [1e-5, 1e-4, 0.001, 0.01, 0.015]\n",
    "MSE = np.zeros((len(n_neurons_list),len(eta_list)))\n",
    "\n",
    "for i in range(len(n_neurons_list)):\n",
    "    for j in range(len(eta_list)):\n",
    "        \n",
    "        n_neurons = n_neurons_list[i]\n",
    "        eta = eta_list[j]\n",
    "\n",
    "        nn = NeuralNetwork(3,regr_cost_grad,random_state=1)\n",
    "        layer1 = Layer(n_neurons,sigmoid,sigmoid_grad)\n",
    "        layer2 = Layer(1,linear,linear_grad)\n",
    "        nn.add_layer(layer1)\n",
    "        nn.add_layer(layer2)\n",
    "        nn.train(Xtrain,ytrain,initial_learning_rate=eta,epochs=100)\n",
    "        MSE[i][j] = mse(nn.feed_forward_out(Xtest),ytest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15,10))\n",
    "sns.heatmap(MSE, xticklabels=eta_list, yticklabels=n_neurons_list, annot=True, annot_kws={\"size\": 8.5}, fmt=\".3f\")\n",
    "plt.xlabel(\"Learning rate\")\n",
    "plt.ylabel(\"Number of neurons\")\n",
    "plt.title(\"MSE grid search for learning rate and number of neurons\")\n",
    "#plt.savefig(\"Grid search for learning rate and epochs.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evoluzione test e training durante le epochs\n",
    "\n",
    "n_epochs=500\n",
    "eta = 0.00001\n",
    "step=10\n",
    "\n",
    "nn = NeuralNetwork(3,regr_cost_grad,random_state=1)\n",
    "layer1 = Layer(30,sigmoid,sigmoid_grad)\n",
    "layer2 = Layer(1,linear,linear_grad)\n",
    "nn.add_layer(layer1)\n",
    "nn.add_layer(layer2)\n",
    "\n",
    "MSE_train = []\n",
    "MSE_test = []\n",
    "\n",
    "for i in range(0,n_epochs,step):\n",
    "    MSE_train.append(mse(nn.feed_forward_out(Xtrain),ytrain))\n",
    "    MSE_test.append(mse(nn.feed_forward_out(Xtest),ytest))\n",
    "    nn.train(Xtrain,ytrain,initial_learning_rate=eta,epochs=step,minibatch_size = 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,8))\n",
    "plt.plot(range(0,n_epochs,step),MSE_train,label='Train')\n",
    "plt.plot(range(0,n_epochs,step),MSE_test,label='Test')\n",
    "plt.legend()\n",
    "plt.title(\"Training and test error vs Epochs\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Error\")\n",
    "plt.ylim(-0.1,2)\n",
    "#plt.savefig(\"train_test_error.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# confronto numero di layer 1,2,3\n",
    "\n",
    "# evoluzione test e training durante le epochs\n",
    "\n",
    "n_epochs = 100\n",
    "eta = 0.0001\n",
    "step = 5\n",
    "\n",
    "nn_1 = NeuralNetwork(3,regr_cost_grad,random_state=1)\n",
    "nn_2 = NeuralNetwork(3,regr_cost_grad,random_state=1)\n",
    "nn_3 = NeuralNetwork(3,regr_cost_grad,random_state=1)\n",
    "layer11 = Layer(30,sigmoid,sigmoid_grad)\n",
    "layer21 = Layer(1,linear,linear_grad)\n",
    "layer12 = Layer(30,sigmoid,sigmoid_grad)\n",
    "layer22 = Layer(30,sigmoid,sigmoid_grad)\n",
    "layer32 = Layer(1,linear,linear_grad)\n",
    "layer13 = Layer(30,sigmoid,sigmoid_grad)\n",
    "layer23 = Layer(30,sigmoid,sigmoid_grad)\n",
    "layer33 = Layer(30,sigmoid,sigmoid_grad)\n",
    "layer43 = Layer(1,linear,linear_grad)\n",
    "\n",
    "nn_1.add_layer(layer11)\n",
    "nn_1.add_layer(layer21)\n",
    "nn_2.add_layer(layer12)\n",
    "nn_2.add_layer(layer22)\n",
    "nn_2.add_layer(layer32)\n",
    "nn_3.add_layer(layer13)\n",
    "nn_3.add_layer(layer23)\n",
    "nn_3.add_layer(layer33)\n",
    "nn_3.add_layer(layer43)\n",
    "\n",
    "MSE_1 = []\n",
    "MSE_2 = []\n",
    "MSE_3 = []\n",
    "\n",
    "\n",
    "for i in range(0,n_epochs,step):\n",
    "    MSE_1.append(mse(nn_1.feed_forward_out(Xtest),ytest))\n",
    "    MSE_2.append(mse(nn_2.feed_forward_out(Xtest),ytest))\n",
    "    MSE_3.append(mse(nn_3.feed_forward_out(Xtest),ytest))\n",
    "    nn_1.train(Xtrain,ytrain,initial_learning_rate=eta,epochs=step,minibatch_size = 20)\n",
    "    nn_2.train(Xtrain,ytrain,initial_learning_rate=eta,epochs=step,minibatch_size = 20)\n",
    "    nn_3.train(Xtrain,ytrain,initial_learning_rate=eta,epochs=step,minibatch_size = 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,8))\n",
    "plt.plot(range(0,n_epochs,step),MSE_1,label='1 hidden layer')\n",
    "plt.plot(range(0,n_epochs,step),MSE_2,label='2 hidden layers')\n",
    "plt.plot(range(0,n_epochs,step),MSE_3,label='3 hidden layers')\n",
    "plt.legend()\n",
    "plt.title(\"Test error vs Epochs\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Error\")\n",
    "plt.ylim(-0.1,2.5)\n",
    "#plt.savefig(\"number_of_hidden_layers.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gd and sgd (change minibatch size)\n",
    "\n",
    "# evoluzione test e training durante le epochs\n",
    "\n",
    "n_epochs = 400\n",
    "eta = 0.00001\n",
    "step = 5\n",
    "\n",
    "nn_gd = NeuralNetwork(3,regr_cost_grad,random_state=1)\n",
    "nn_sgd1 = NeuralNetwork(3,regr_cost_grad,random_state=1)\n",
    "nn_sgd2 = NeuralNetwork(3,regr_cost_grad,random_state=1)\n",
    "nn_sgd3 = NeuralNetwork(3,regr_cost_grad,random_state=1)\n",
    "layer1gd = Layer(30,sigmoid,sigmoid_grad)\n",
    "layer2gd = Layer(1,linear,linear_grad)\n",
    "layer1sgd1 = Layer(30,sigmoid,sigmoid_grad)\n",
    "layer2sgd1 = Layer(1,linear,linear_grad)\n",
    "layer1sgd2 = Layer(30,sigmoid,sigmoid_grad)\n",
    "layer2sgd2 = Layer(1,linear,linear_grad)\n",
    "layer1sgd3 = Layer(30,sigmoid,sigmoid_grad)\n",
    "layer2sgd3 = Layer(1,linear,linear_grad)\n",
    "nn_gd.add_layer(layer1gd)\n",
    "nn_gd.add_layer(layer2gd)\n",
    "nn_sgd1.add_layer(layer1sgd1)\n",
    "nn_sgd1.add_layer(layer2sgd1)\n",
    "nn_sgd2.add_layer(layer1sgd2)\n",
    "nn_sgd2.add_layer(layer2sgd2)\n",
    "nn_sgd3.add_layer(layer1sgd3)\n",
    "nn_sgd3.add_layer(layer2sgd3)\n",
    "\n",
    "MSE_gd = []\n",
    "MSE_sgd1 = []\n",
    "MSE_sgd2 = []\n",
    "MSE_sgd3 = []\n",
    "\n",
    "for i in range(0,n_epochs,step):\n",
    "    MSE_gd.append(mse(nn_gd.feed_forward_out(Xtest),ytest))\n",
    "    MSE_sgd1.append(mse(nn_sgd1.feed_forward_out(Xtest),ytest))\n",
    "    MSE_sgd2.append(mse(nn_sgd2.feed_forward_out(Xtest),ytest))\n",
    "    MSE_sgd3.append(mse(nn_sgd3.feed_forward_out(Xtest),ytest))\n",
    "    nn_gd.train(Xtrain,ytrain,initial_learning_rate=eta,epochs=step,minibatch_size = 1000)\n",
    "    nn_sgd1.train(Xtrain,ytrain,initial_learning_rate=eta,epochs=step,minibatch_size = 50)\n",
    "    nn_sgd2.train(Xtrain,ytrain,initial_learning_rate=eta,epochs=step,minibatch_size = 20)\n",
    "    nn_sgd3.train(Xtrain,ytrain,initial_learning_rate=eta,epochs=step,minibatch_size = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epochs = 400\n",
    "eta = 0.00001\n",
    "step = 5\n",
    "\n",
    "plt.figure(figsize=(10,8))\n",
    "plt.plot(range(0,n_epochs,step),MSE_gd,label='GD')\n",
    "plt.plot(range(0,n_epochs,step),MSE_sgd1,label='SGD minibatch_size=50')\n",
    "plt.plot(range(0,n_epochs,step),MSE_sgd2,label='SGD minibatch_size=20')\n",
    "plt.plot(range(0,n_epochs,step),MSE_sgd3,label='SGD minibatch_size=1')\n",
    "plt.legend()\n",
    "plt.title(\"Test error vs Epochs\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Error\")\n",
    "plt.ylim(-0.1,2)\n",
    "plt.savefig(\"GD vs SGD test error\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sigmoid\n",
    "# grid search regularization and learning rate\n",
    "\n",
    "eta_list = [1e-5, 1e-4, 0.001, 0.01, 0.015]\n",
    "reg_list = [0, 0.01, 0.1, 1, 10]\n",
    "MSE_sigm = np.zeros((len(reg_list),len(eta_list)))\n",
    "\n",
    "for j in range(len(eta_list)):\n",
    "    for i in range(len(reg_list)):\n",
    "        \n",
    "        reg = reg_list[i]\n",
    "        eta = eta_list[j]\n",
    "\n",
    "        nn = NeuralNetwork(3,regr_cost_grad,random_state=1)\n",
    "        layer1 = Layer(30,sigmoid,sigmoid_grad)\n",
    "        layer2 = Layer(1,linear,linear_grad)\n",
    "        nn.add_layer(layer1)\n",
    "        nn.add_layer(layer2)\n",
    "        nn.train(Xtrain,ytrain,initial_learning_rate=eta,regularization=reg, epochs=200)\n",
    "        MSE_sigm[i][j] = mse(nn.feed_forward_out(Xtest),ytest)\n",
    "\n",
    "# best results for 0.0001 and 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#relu\n",
    "# grid search regularization and learning rate\n",
    "\n",
    "eta_list = [1e-5, 1e-4, 0.001, 0.01, 0.015]\n",
    "reg_list = [0, 0.01, 0.1, 1, 10]\n",
    "MSE_relu = np.zeros((len(reg_list),len(eta_list)))\n",
    "\n",
    "for j in range(len(eta_list)):\n",
    "    for i in range(len(reg_list)):\n",
    "        \n",
    "        reg = reg_list[i]\n",
    "        eta = eta_list[j]\n",
    "\n",
    "        nn = NeuralNetwork(3,regr_cost_grad,random_state=1)\n",
    "        layer1 = Layer(30,ReLU,ReLU_grad)\n",
    "        layer2 = Layer(1,linear,linear_grad)\n",
    "        nn.add_layer(layer1)\n",
    "        nn.add_layer(layer2)\n",
    "        nn.train(Xtrain,ytrain,initial_learning_rate=eta,regularization=reg, epochs=200)\n",
    "        MSE_relu[i][j] = mse(nn.feed_forward_out(Xtest),ytest)\n",
    "\n",
    "# best results for 0.0001 and 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#leakyrelu\n",
    "# grid search regularization and learning rate\n",
    "\n",
    "eta_list = [1e-5, 1e-4, 0.001, 0.01, 0.015]\n",
    "reg_list = [0, 0.01, 0.1, 1, 10]\n",
    "MSE_lrelu = np.zeros((len(reg_list),len(eta_list)))\n",
    "\n",
    "for j in range(len(eta_list)):\n",
    "    for i in range(len(reg_list)):\n",
    "        \n",
    "        reg = reg_list[i]\n",
    "        eta = eta_list[j]\n",
    "\n",
    "        nn = NeuralNetwork(3,regr_cost_grad,random_state=1)\n",
    "        layer1 = Layer(30,leakyReLU,leakyReLU_grad)\n",
    "        layer2 = Layer(1,linear,linear_grad)\n",
    "        nn.add_layer(layer1)\n",
    "        nn.add_layer(layer2)\n",
    "        nn.train(Xtrain,ytrain,initial_learning_rate=eta,regularization=reg, epochs=200)\n",
    "        MSE_lrelu[i][j] = mse(nn.feed_forward_out(Xtest),ytest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tanh\n",
    "# grid search regularization and learning rate\n",
    "\n",
    "eta_list = [1e-5, 1e-4, 0.001, 0.01, 0.015]\n",
    "reg_list = [0, 0.01, 0.1, 1, 10]\n",
    "MSE_tanh = np.zeros((len(reg_list),len(eta_list)))\n",
    "\n",
    "for j in range(len(eta_list)):\n",
    "    for i in range(len(reg_list)):\n",
    "       \n",
    "        reg = reg_list[i]\n",
    "        eta = eta_list[j]\n",
    "\n",
    "        nn = NeuralNetwork(3,regr_cost_grad,random_state=1)\n",
    "        layer1 = Layer(30,tanh,tanh_grad)\n",
    "        layer2 = Layer(1,linear,linear_grad)\n",
    "        nn.add_layer(layer1)\n",
    "        nn.add_layer(layer2)\n",
    "        nn.train(Xtrain,ytrain,initial_learning_rate=eta,regularization=reg, epochs=200)\n",
    "        MSE_tanh[i][j] = mse(nn.feed_forward_out(Xtest),ytest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15,10))\n",
    "sns.heatmap(MSE_sigm, xticklabels=eta_list, yticklabels=reg_list, annot=True, annot_kws={\"size\": 8.5}, fmt=\".3f\")\n",
    "plt.ylabel(\"Regularization parameter\")\n",
    "plt.xlabel(\"Learning rate\")\n",
    "plt.title(\"MSE grid search for learning rate and regularization parameter\")\n",
    "#plt.savefig(\"sigmoid_grid_search.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15,10))\n",
    "sns.heatmap(MSE_relu, vmin=0.007, vmax=9.405, xticklabels=eta_list, yticklabels=reg_list, annot=True, annot_kws={\"size\": 8.5}, fmt=\".3f\")\n",
    "plt.ylabel(\"Regularization parameter\")\n",
    "plt.xlabel(\"Learning rate\")\n",
    "plt.title(\"MSE grid search for learning rate and regularization parameter\")\n",
    "#plt.savefig(\"RELU_Grid_search.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15,10))\n",
    "sns.heatmap(MSE_lrelu, vmin=0.007, vmax=9.405, xticklabels=eta_list, yticklabels=reg_list, annot=True, annot_kws={\"size\": 8.5}, fmt=\".3f\")\n",
    "plt.ylabel(\"Regularization parameter\")\n",
    "plt.xlabel(\"Learning rate\")\n",
    "plt.title(\"MSE grid search for learning rate and regularization parameter\")\n",
    "plt.savefig(\"lrelu_grid_search.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15,10))\n",
    "sns.heatmap(MSE_tanh, xticklabels=eta_list, yticklabels=reg_list, annot=True, annot_kws={\"size\": 8.5}, fmt=\".3f\")\n",
    "plt.ylabel(\"Regularization parameter\")\n",
    "plt.xlabel(\"Learning rate\")\n",
    "plt.title(\"MSE grid search for learning rate and regularization parameter\")\n",
    "plt.savefig(\"tanh_grid_search.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# different learning rate schedules\n",
    "\n",
    "# evoluzione test e training durante le epochs\n",
    "\n",
    "n_epochs = 100\n",
    "eta_const = 0.0001\n",
    "init_eta = 0.01\n",
    "final_eta = 0.0001\n",
    "step = 5\n",
    "\n",
    "nn_1 = NeuralNetwork(3,regr_cost_grad,random_state=1)\n",
    "nn_2 = NeuralNetwork(3,regr_cost_grad,random_state=1)\n",
    "layer11 = Layer(30,sigmoid,sigmoid_grad)\n",
    "layer21 = Layer(1,linear,linear_grad)\n",
    "layer12 = Layer(30,sigmoid,sigmoid_grad)\n",
    "layer22 = Layer(1,linear,linear_grad)\n",
    "\n",
    "nn_1.add_layer(layer11)\n",
    "nn_1.add_layer(layer21)\n",
    "nn_2.add_layer(layer12)\n",
    "nn_2.add_layer(layer22)\n",
    "\n",
    "MSE_1 = []\n",
    "MSE_2 = []\n",
    "\n",
    "for i in range(0,n_epochs,step):\n",
    "    MSE_1.append(mse(nn_1.feed_forward_out(Xtest),ytest))\n",
    "    MSE_2.append(mse(nn_2.feed_forward_out(Xtest),ytest))\n",
    "\n",
    "    nn_1.train(Xtrain,ytrain,initial_learning_rate=eta_const,epochs=step,minibatch_size = 20)\n",
    "\n",
    "    eta = init_eta + (final_eta-init_eta)*i/n_epochs\n",
    "    nn_2.train(Xtrain,ytrain,initial_learning_rate=0.01, epochs=step,minibatch_size = 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,8))\n",
    "plt.plot(range(0,n_epochs,step),MSE_1,label='Constant learning rate')\n",
    "plt.plot(range(0,n_epochs,step),MSE_2,label='Adapting learning rate')\n",
    "plt.legend()\n",
    "plt.title(\"Test error vs Epochs\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Error\")\n",
    "plt.ylim(-0.1,1)\n",
    "#plt.savefig(\"constant_adapting_lr.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# comparison with previous code and with tensorflow/keras functions\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(30, activation='sigmoid'))\n",
    "model.add(Dense(1, activation='linear'))\n",
    "\n",
    "model.compile(optimizer=tf.optimizers.Adam(learning_rate=0.001), loss='mean_squared_error')\n",
    "model.fit(Xtrain,ytrain,epochs=500,verbose=0)\n",
    "\n",
    "tf_error = model.evaluate(Xtest,ytest)\n",
    "\n",
    "dnn = MLPRegressor(hidden_layer_sizes=(30,),learning_rate_init=0.001, max_iter=500)\n",
    "dnn.fit(Xtrain,ytrain)\n",
    "test_predict = dnn.predict(Xtest)\n",
    "\n",
    "sklearn_error = mse(test_predict,ytest)\n",
    "\n",
    "nn = NeuralNetwork(3,regr_cost_grad,random_state=1)\n",
    "layer1 = Layer(30,sigmoid,sigmoid_grad)\n",
    "layer2 = Layer(1,linear,linear_grad)\n",
    "nn.add_layer(layer1)\n",
    "nn.add_layer(layer2)\n",
    "nn.train(Xtrain,ytrain,initial_learning_rate=0.001,regularization=0, epochs=500)\n",
    "nn_error = mse(nn.feed_forward_out(Xtest),ytest)\n",
    "\n",
    "print(\"Tensorflow error: \" + str(tf_error))\n",
    "print(\"Scikit-learn error: \" + str(sklearn_error))\n",
    "print(\"Our model's error: \" + str(nn_error))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# weights con variance diverse\n",
    "# bias con normal distribution o fissi\n",
    "\n",
    "# evoluzione test e training durante le epochs\n",
    "\n",
    "n_epochs = 100\n",
    "eta=0.0001\n",
    "step = 5\n",
    "\n",
    "nn = NeuralNetwork(3,regr_cost_grad,random_state=1)\n",
    "layer1 = Layer(30,sigmoid,sigmoid_grad)\n",
    "layer2 = Layer(1,linear,linear_grad)\n",
    "\n",
    "nn.add_layer(layer1)\n",
    "nn.add_layer(layer2)\n",
    "\n",
    "var1_err=[]\n",
    "varn_err = []\n",
    "\n",
    "for i in range(0,n_epochs,step):\n",
    "    varn_err.append(mse(nn.feed_forward_out(Xtest),ytest))\n",
    "    nn.train(Xtrain,ytrain,initial_learning_rate=eta,epochs=step,minibatch_size = 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,8))\n",
    "plt.plot(range(0,n_epochs,step),var1_err,label='Variance 1')\n",
    "plt.plot(range(0,n_epochs,step),varn_err,label='Variance 1/sqrt(n)')\n",
    "plt.legend()\n",
    "plt.title(\"Test error vs Epochs\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Error\")\n",
    "plt.ylim(-0.1,1)\n",
    "#plt.savefig(\"weight_variance.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# different bias initialization\n",
    "\n",
    "# evoluzione test e training durante le epochs\n",
    "\n",
    "n_epochs = 100\n",
    "eta=0.001\n",
    "step = 5\n",
    "\n",
    "nn = NeuralNetwork(3,regr_cost_grad,random_state=1)\n",
    "layer1 = Layer(30,sigmoid,sigmoid_grad,initial_bias=1)\n",
    "layer2 = Layer(1,linear,linear_grad, initial_bias=1)\n",
    "\n",
    "nn.add_layer(layer1)\n",
    "nn.add_layer(layer2)\n",
    "\n",
    "b5_err = []\n",
    "\n",
    "for i in range(0,n_epochs,step):\n",
    "    b5_err.append(mse(nn.feed_forward_out(Xtest),ytest))\n",
    "    nn.train(Xtrain,ytrain,initial_learning_rate=eta_const,epochs=step,minibatch_size = 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,8))\n",
    "plt.plot(range(0,n_epochs,step),b1_err,label='Initial bias 0.01')\n",
    "plt.plot(range(0,n_epochs,step),b2_err,label='Initial bias 0.1')\n",
    "plt.plot(range(0,n_epochs,step),b4_err,label='Initial bias 1')\n",
    "plt.plot(range(0,n_epochs,step),b5_err,label='Standard normal bias')\n",
    "plt.legend()\n",
    "plt.title(\"Test error vs Epochs\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Error\")\n",
    "plt.ylim(-0.1,1)\n",
    "#plt.savefig(\"bias_init.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# different activation functions\n",
    "# evoluzione test e training durante le epochs\n",
    "\n",
    "n_epochs = 400\n",
    "step = 5\n",
    "\n",
    "nn_sigm = NeuralNetwork(3,regr_cost_grad,random_state=1)\n",
    "nn_relu = NeuralNetwork(3,regr_cost_grad,random_state=1)\n",
    "nn_lrelu = NeuralNetwork(3,regr_cost_grad,random_state=1)\n",
    "nn_tanh = NeuralNetwork(3,regr_cost_grad,random_state=1)\n",
    "layer1sigm = Layer(30,sigmoid,sigmoid_grad)\n",
    "layer2sigm= Layer(1,linear,linear_grad)\n",
    "layer1relu = Layer(30,ReLU,ReLU_grad)\n",
    "layer2relu = Layer(1,linear,linear_grad)\n",
    "layer1lrelu = Layer(30,leakyReLU,leakyReLU_grad)\n",
    "layer2lrelu = Layer(1,linear,linear_grad)\n",
    "layer1tanh = Layer(30,tanh,tanh_grad)\n",
    "layer2tanh = Layer(1,linear,linear_grad)\n",
    "nn_sigm.add_layer(layer1sigm)\n",
    "nn_sigm.add_layer(layer2sigm)\n",
    "nn_relu.add_layer(layer1relu)\n",
    "nn_relu.add_layer(layer2relu)\n",
    "nn_lrelu.add_layer(layer1lrelu)\n",
    "nn_lrelu.add_layer(layer2lrelu)\n",
    "nn_tanh.add_layer(layer1tanh)\n",
    "nn_tanh.add_layer(layer2tanh)\n",
    "\n",
    "err_sigm = []\n",
    "err_relu = []\n",
    "err_lrelu = []\n",
    "err_tanh = []\n",
    "\n",
    "for i in range(0,n_epochs,step):\n",
    "    err_sigm.append(mse(nn_sigm.feed_forward_out(Xtest),ytest))\n",
    "    err_relu.append(mse(nn_relu.feed_forward_out(Xtest),ytest))\n",
    "    err_lrelu.append(mse(nn_lrelu.feed_forward_out(Xtest),ytest))\n",
    "    err_tanh.append(mse(nn_tanh.feed_forward_out(Xtest),ytest))\n",
    "    nn_sigm.train(Xtrain,ytrain,initial_learning_rate=0.00001,epochs=step)\n",
    "    nn_relu.train(Xtrain,ytrain,initial_learning_rate=0.00001,epochs=step)\n",
    "    nn_lrelu.train(Xtrain,ytrain,initial_learning_rate=0.00001,epochs=step)\n",
    "    nn_tanh.train(Xtrain,ytrain,initial_learning_rate=0.00001,epochs=step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,8))\n",
    "plt.plot(range(0,n_epochs,step),err_sigm,label='Sigmoid')\n",
    "plt.plot(range(0,n_epochs,step),err_relu,label='ReLU')\n",
    "plt.plot(range(0,n_epochs,step),err_lrelu,label='Leaky ReLU')\n",
    "plt.plot(range(0,n_epochs,step),err_tanh,label='Tanh')\n",
    "plt.legend()\n",
    "plt.title(\"Test error vs Epochs\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Error\")\n",
    "plt.ylim(-0.05,1)\n",
    "plt.savefig(\"nn_activation_function.png\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.7 ('uni')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "418c59836fa3b9ce9db29957e801ce461de12293cee0e24ddf0e9f7bd846be9f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
