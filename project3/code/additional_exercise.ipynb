{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from random import random, seed\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import r2_score as r2, mean_squared_error as mse\n",
    "from sklearn.utils import resample\n",
    "import pandas as pd\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.linear_model import Ridge\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.ensemble import AdaBoostRegressor\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bootstrap_bias_variance_decomposition(data_train, data_test, z_train, z_test, model, n_boostraps):\n",
    "\n",
    "    \"\"\" computes the bias-variance decomposition of MSE through boostrapping \"\"\"\n",
    "\n",
    "    z_pred = np.empty((len(z_test), n_boostraps))\n",
    "    for i in range(n_boostraps):\n",
    "        data_, z_ = resample(data_train, z_train)\n",
    "        model.fit(data_,z_)\n",
    "        z_pred[:,i] = np.squeeze(model.predict(data_test)).T\n",
    "\n",
    "    error = np.mean( np.mean((z_test - z_pred)**2, axis=1, keepdims=True) )\n",
    "    bias = np.mean( (z_test - np.mean(z_pred, axis=1, keepdims=True))**2 )\n",
    "    variance = np.mean( np.var(z_pred, axis=1, keepdims=True) )\n",
    "\n",
    "    return error, bias, variance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create data set.\n",
    "num_points = 100\n",
    "x = np.linspace(-3, 3, num_points).reshape(-1, 1)\n",
    "y = np.exp(-x**2) + 1.5 * np.exp(-(x-2)**2) + np.random.normal(0, 0.1, x.shape)\n",
    "n_boostraps = 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### OLS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_deg = 20\n",
    "model = LinearRegression(fit_intercept=False)\n",
    "error1 = np.zeros(max_deg)\n",
    "bias1 = np.zeros(max_deg)\n",
    "variance1 = np.zeros(max_deg)\n",
    "\n",
    "# changing the degree of the polynomia\n",
    "for deg in range(max_deg):\n",
    "\n",
    "    poly = PolynomialFeatures(degree=deg+1)\n",
    "    data = poly.fit_transform(x)\n",
    "\n",
    "    data_train, data_test, z_train, z_test = train_test_split(data, y, test_size=0.2)\n",
    "\n",
    "    error1[deg], bias1[deg], variance1[deg] = bootstrap_bias_variance_decomposition(data_train, data_test, z_train, z_test, model, n_boostraps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(10,8))\n",
    "plt.plot(range(1,max_deg+1), error1, label='MSE')\n",
    "plt.plot(range(1,max_deg+1), bias1, label='Bias')\n",
    "plt.plot(range(1,max_deg+1), variance1, label='Variance')\n",
    "plt.title(\"Changing degree for OLS\", fontsize=15)\n",
    "plt.xlabel(\"Polynomial degree\", fontsize=15)\n",
    "plt.xticks(range(1,max_deg+1))\n",
    "plt.ylabel(\"Error\", fontsize=15)\n",
    "plt.legend(fontsize=15)\n",
    "plt.savefig('Changing degree for OLS.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ridge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "deg = 16\n",
    "lambda_v = np.logspace(-8,1,50)\n",
    "\n",
    "poly = PolynomialFeatures(degree=deg+1)\n",
    "data = poly.fit_transform(x)\n",
    "\n",
    "data_train, data_test, z_train, z_test = train_test_split(data, y, test_size=0.2)\n",
    "\n",
    "error2 = np.zeros(len(lambda_v))\n",
    "bias2 = np.zeros(len(lambda_v))\n",
    "variance2 = np.zeros(len(lambda_v))\n",
    "\n",
    "for i in range(len(lambda_v)):\n",
    "\n",
    "    lam = lambda_v[i]\n",
    "    model = Ridge(lam,fit_intercept=False)\n",
    "\n",
    "    error2[i], bias2[i], variance2[i] = bootstrap_bias_variance_decomposition(data_train, data_test, z_train, z_test, model, n_boostraps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(10,8))\n",
    "plt.plot(np.log(lambda_v), error2, label='MSE')\n",
    "plt.plot(np.log(lambda_v), bias2, label='Bias')\n",
    "plt.plot(np.log(lambda_v), variance2, label='Variance')\n",
    "plt.title(\"Changing lambda for Ridge regression\", fontsize=15)\n",
    "plt.xlabel(\"log(lambda)\", fontsize=15)\n",
    "plt.ylabel(\"Error\", fontsize=15)\n",
    "plt.legend(fontsize=15)\n",
    "plt.savefig('Changing lambda for Ridge.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lasso"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "deg = 12\n",
    "lambda_v = np.logspace(-15,2,50)\n",
    "#lambda_v=[]\n",
    "\n",
    "poly = PolynomialFeatures(degree=deg+1)\n",
    "data = poly.fit_transform(x)\n",
    "\n",
    "data_train, data_test, z_train, z_test = train_test_split(data, y, test_size=0.2)\n",
    "\n",
    "error3 = np.zeros(len(lambda_v))\n",
    "bias3 = np.zeros(len(lambda_v))\n",
    "variance3 = np.zeros(len(lambda_v))\n",
    "\n",
    "for i in range(len(lambda_v)):\n",
    "\n",
    "    lam = lambda_v[i]\n",
    "    model = Lasso(lam,fit_intercept=False)\n",
    "\n",
    "    error3[i], bias3[i], variance3[i] = bootstrap_bias_variance_decomposition(data_train, data_test, z_train, z_test, model, n_boostraps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(10,8))\n",
    "plt.plot(np.log(lambda_v), error3, label='MSE')\n",
    "plt.plot(np.log(lambda_v), bias3, label='Bias')\n",
    "plt.plot(np.log(lambda_v), variance3, label='Variance')\n",
    "plt.title(\"Changing lambda for Lasso regression\", fontsize=15)\n",
    "plt.xlabel(\"log(lambda)\", fontsize=15)\n",
    "plt.ylabel(\"Error\", fontsize=15)\n",
    "plt.legend(fontsize=15)\n",
    "#plt.savefig('Changing lambda for Lasso regression.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tree-based methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regression tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_depth = 10\n",
    "\n",
    "data_train, data_test, z_train, z_test = train_test_split(x,y, test_size=0.2)\n",
    "\n",
    "model = DecisionTreeRegressor()\n",
    "\n",
    "error4 = np.zeros(max_depth)\n",
    "bias4 = np.zeros(max_depth)\n",
    "variance4 = np.zeros(max_depth)\n",
    "\n",
    "for i in range(max_depth):\n",
    "    model.max_depth = i+1\n",
    "    error4[i], bias4[i], variance4[i] = bootstrap_bias_variance_decomposition(data_train, data_test, z_train, z_test, model, n_boostraps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(10,8))\n",
    "plt.plot(range(1,max_depth+1), error4, label='MSE')\n",
    "plt.plot(range(1,max_depth+1), bias4, label='Bias')\n",
    "plt.plot(range(1,max_depth+1), variance4, label='Variance')\n",
    "plt.title(\"Changing maximum depth for Regression Tree\", fontsize=15)\n",
    "plt.xlabel(\"Depth\", fontsize=15)\n",
    "plt.ylabel(\"Error\", fontsize=15)\n",
    "plt.legend(fontsize=15)\n",
    "plt.savefig('Changing maximum depth for Regression Tree')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_points = 100\n",
    "\n",
    "# Make data set.\n",
    "x = np.linspace(-3, 3, num_points).reshape(-1, 1)\n",
    "y = np.exp(-x**2) + 1.5 * np.exp(-(x-2)**2) + np.random.normal(0, 0.1, x.shape)\n",
    "\n",
    "n_boostraps = 100\n",
    "ntree_v = range(1,101,10)\n",
    "\n",
    "data_train, data_test, z_train, z_test = train_test_split(x, y, test_size=0.2)\n",
    "\n",
    "error5 = np.zeros(len(ntree_v))\n",
    "bias5 = np.zeros(len(ntree_v))\n",
    "variance5 = np.zeros(len(ntree_v))\n",
    "\n",
    "for i in range(len(ntree_v)):\n",
    "\n",
    "    ntree = ntree_v[i]\n",
    "    model = RandomForestRegressor(n_estimators=ntree, verbose=False)\n",
    "    error5[i], bias5[i], variance5[i] = bootstrap_bias_variance_decomposition(data_train, data_test, z_train, z_test, model, n_boostraps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(10,8))\n",
    "plt.plot(ntree_v, error5, label='MSE')\n",
    "plt.plot(ntree_v, bias5, label='Bias')\n",
    "plt.plot(ntree_v, variance5, label='Variance')\n",
    "plt.title(\"Changing number of trees for Random Forest\", fontsize=15)\n",
    "plt.xlabel(\"Number of trees\", fontsize=15)\n",
    "plt.ylabel(\"Error\", fontsize=15)\n",
    "plt.legend(fontsize=15)\n",
    "#plt.savefig('Changing number of trees for Random Forest.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m_v = range(1,13)\n",
    "poly = PolynomialFeatures(12)\n",
    "data = poly.fit_transform(x)\n",
    "\n",
    "data_train, data_test, z_train, z_test = train_test_split(data, y, test_size=0.2)\n",
    "\n",
    "error6 = np.zeros(len(m_v))\n",
    "bias6 = np.zeros(len(m_v))\n",
    "variance6 = np.zeros(len(m_v))\n",
    "\n",
    "for i in range(len(m_v)):\n",
    "\n",
    "    m = m_v[i]\n",
    "    model = RandomForestRegressor(max_features=m)\n",
    "    error6[i], bias6[i], variance6[i] = bootstrap_bias_variance_decomposition(data_train, data_test, z_train, z_test, model, n_boostraps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(10,8))\n",
    "plt.plot(m_v, error6, label='MSE')\n",
    "plt.plot(m_v, bias6, label='Bias')\n",
    "plt.plot(m_v, variance6, label='Variance')\n",
    "plt.title(\"Changing number of predictors at each split for Random Forest\", fontsize=15)\n",
    "plt.xlabel(\"Number of predictors\", fontsize=15)\n",
    "plt.ylabel(\"Error\", fontsize=15)\n",
    "plt.legend(fontsize=15)\n",
    "#plt.savefig('Changing number of predictors at each split for Random Forest.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AdaBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ntree_v = range(1,101,10)\n",
    "\n",
    "num_points = 50\n",
    "\n",
    "# Make data set.\n",
    "x = np.linspace(-3, 3, num_points).reshape(-1, 1)\n",
    "y = np.exp(-x**2) + 1.5 * np.exp(-(x-2)**2) + np.random.normal(0, 0.1, x.shape)\n",
    "\n",
    "n_boostraps = 20\n",
    "\n",
    "data_train, data_test, z_train, z_test = train_test_split(x, y, test_size=0.2)\n",
    "\n",
    "error7 = np.zeros(len(ntree_v))\n",
    "bias7 = np.zeros(len(ntree_v))\n",
    "variance7 = np.zeros(len(ntree_v))\n",
    "\n",
    "for i in range(len(ntree_v)):\n",
    "\n",
    "    ntree = ntree_v[i]\n",
    "    model = AdaBoostRegressor(n_estimators=ntree)\n",
    "    error7[i], bias7[i], variance7[i] = bootstrap_bias_variance_decomposition(data_train, data_test, z_train, z_test, model, n_boostraps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(10,8))\n",
    "plt.plot(ntree_v, error7, label='MSE')\n",
    "plt.plot(ntree_v, bias7, label='Bias')\n",
    "plt.plot(ntree_v, variance7, label='Variance')\n",
    "plt.title(\"Changing number of trees for AdaBoost\", fontsize=15)\n",
    "plt.xlabel(\"Number of trees\", fontsize=15)\n",
    "plt.ylabel(\"Error\", fontsize=15)\n",
    "plt.legend(fontsize=15)\n",
    "#plt.savefig('Changing number of trees for AdaBoost.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FFNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_neurons_v = [1,2,5,10,20,30]\n",
    "\n",
    "num_points = 50\n",
    "\n",
    "# Make data set.\n",
    "x = np.linspace(-3, 3, num_points).reshape(-1, 1)\n",
    "y = np.exp(-x**2) + 1.5 * np.exp(-(x-2)**2) + np.random.normal(0, 0.1, x.shape)\n",
    "\n",
    "n_boostraps = 20\n",
    "\n",
    "data_train, data_test, z_train, z_test = train_test_split(x, y, test_size=0.2)\n",
    "\n",
    "error8 = np.zeros(len(n_neurons_v))\n",
    "bias8 = np.zeros(len(n_neurons_v))\n",
    "variance8 = np.zeros(len(n_neurons_v))\n",
    "\n",
    "for i in range(len(n_neurons_v)):\n",
    "        \n",
    "        n_neurons = n_neurons_v[i]\n",
    "        model = MLPRegressor(hidden_layer_sizes=(n_neurons,),learning_rate_init=0.001, max_iter=500)\n",
    "        error8[i], bias8[i], variance8[i] = bootstrap_bias_variance_decomposition(data_train, data_test, z_train, z_test, model, n_boostraps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(10,8))\n",
    "plt.plot(n_neurons_v, error8, label='MSE')\n",
    "plt.plot(n_neurons_v, bias8, label='Bias')\n",
    "plt.plot(n_neurons_v, variance8, label='Variance')\n",
    "plt.title(\"Bias-Variance tradeoff with changing number of neurons for NN\", fontsize=15)\n",
    "plt.xlabel(\"Number of neurons\", fontsize=15)\n",
    "plt.ylabel(\"Error\", fontsize=15)\n",
    "plt.legend(fontsize=15)\n",
    "plt.savefig('Changing number of neurons for NN.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_layers_v = [1,2,3,4,5,6,7,8,9,10,11,12,13]\n",
    "\n",
    "num_points = 50\n",
    "\n",
    "# Make data set.\n",
    "x = np.linspace(-3, 3, num_points).reshape(-1, 1)\n",
    "y = np.exp(-x**2) + 1.5 * np.exp(-(x-2)**2) + np.random.normal(0, 0.1, x.shape)\n",
    "\n",
    "n_boostraps = 20\n",
    "\n",
    "data_train, data_test, z_train, z_test = train_test_split(x, y, test_size=0.2)\n",
    "\n",
    "error9 = np.zeros(len(n_layers_v))\n",
    "bias9 = np.zeros(len(n_layers_v))\n",
    "variance9 = np.zeros(len(n_layers_v))\n",
    "\n",
    "for i in range(len(n_layers_v)):\n",
    "        \n",
    "        n_layers = n_layers_v[i]\n",
    "        model = MLPRegressor(hidden_layer_sizes=[20]*n_layers,learning_rate_init=0.001, max_iter=500)\n",
    "        error9[i], bias9[i], variance9[i] = bootstrap_bias_variance_decomposition(data_train, data_test, z_train, z_test, model, n_boostraps)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(10,8))\n",
    "plt.plot(n_layers_v, error9, label='MSE')\n",
    "plt.plot(n_layers_v, bias9, label='Bias')\n",
    "plt.plot(n_layers_v, variance9, label='Variance')\n",
    "plt.title(\"Bias-Variance tradeoff with changing number of layers for NN\", fontsize=15)\n",
    "plt.xlabel(\"Number of layers\", fontsize=15)\n",
    "plt.ylabel(\"Error\", fontsize=15)\n",
    "plt.legend(fontsize=15)\n",
    "plt.savefig('Changing number of layers for NN.png')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.7 ('uni')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "418c59836fa3b9ce9db29957e801ce461de12293cee0e24ddf0e9f7bd846be9f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
