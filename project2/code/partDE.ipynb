{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing various packages\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "from activation_functions import *\n",
    "from optimizer_functions import *\n",
    "from cost_functions import *\n",
    "from Layer import Layer\n",
    "from NeuralNetwork import NeuralNetwork"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# same analysis with classification\n",
    "\n",
    "# importing the wisconsin cancer dataset\n",
    "\n",
    "import sklearn.datasets\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "bunch = sklearn.datasets.load_breast_cancer()\n",
    "X = bunch['data']\n",
    "y = bunch['target'].reshape((X.shape[0],1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dividing in train and test\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.2, random_state=1)\n",
    "\n",
    "# normalizing all predictors\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(X_train)\n",
    "X_train = scaler.transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "y_train_ = y_train.reshape((len(y_train),))\n",
    "y_test_ = y_test.reshape((len(y_test),))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tuning learning rate and regularization parameter\n",
    "\n",
    "eta_list = [1e-4, 0.001, 0.01, 0.02, 0.05]\n",
    "reg_list = [0, 0.01, 0.1, 1, 10]\n",
    "accuracy = np.zeros((len(reg_list),len(eta_list)))\n",
    "\n",
    "for j in range(len(eta_list)):\n",
    "    for i in range(len(reg_list)):\n",
    "       \n",
    "        reg = reg_list[i]\n",
    "        eta = eta_list[j]\n",
    "\n",
    "        nn = NeuralNetwork(30,class_cost_grad,random_state=1)\n",
    "        layer1 = Layer(30, sigmoid, sigmoid_grad)\n",
    "        layer2 = Layer(1, sigmoid, sigmoid_grad)\n",
    "        nn.add_layer(layer1)\n",
    "        nn.add_layer(layer2)\n",
    "        nn.train(X_train,y_train,eta=eta,regularization=reg, epochs=200)\n",
    "\n",
    "        pred = nn.feed_forward_out(X_test)\n",
    "        pred = pred.round()\n",
    "        accuracy[i][j] = np.sum(pred == y_test)/y_test.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15,10))\n",
    "sns.heatmap(accuracy, xticklabels=eta_list, yticklabels=reg_list, annot=True, annot_kws={\"size\": 12}, fmt=\".3f\")\n",
    "plt.ylabel(\"Regularization parameter\", fontsize=15)\n",
    "plt.xlabel(\"Learning rate\", fontsize=15)\n",
    "plt.title(\"Accuracy grid search for learning rate and regularization parameter\", fontsize=15)\n",
    "plt.savefig(\"accuracy_grid_search.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tuning learning rate and number of neurons\n",
    "\n",
    "eta_list = [1e-4, 0.001, 0.01, 0.02, 0.05]\n",
    "n_neurons_list = [10, 20, 30, 50, 75]\n",
    "accuracy = np.zeros((len(reg_list),len(n_neurons_list)))\n",
    "\n",
    "for j in range(len(eta_list)):\n",
    "    for i in range(len(n_neurons_list)):\n",
    "       \n",
    "        n_neurons = n_neurons_list[i]\n",
    "        eta = eta_list[j]\n",
    "\n",
    "        nn = NeuralNetwork(30,class_cost_grad,random_state=1)\n",
    "        layer1 = Layer(n_neurons, sigmoid, sigmoid_grad)\n",
    "        layer2 = Layer(1, sigmoid, sigmoid_grad)\n",
    "        nn.add_layer(layer1)\n",
    "        nn.add_layer(layer2)\n",
    "        nn.train(X_train,y_train,eta=eta, epochs=200)\n",
    "\n",
    "        pred = nn.feed_forward_out(X_test)\n",
    "        pred = pred.round()\n",
    "        accuracy[i][j] = np.sum(pred == y_test)/y_test.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15,10))\n",
    "sns.heatmap(accuracy, xticklabels=eta_list, yticklabels=n_neurons_list, annot=True, annot_kws={\"size\": 8.5}, fmt=\".3f\")\n",
    "plt.ylabel(\"Number of neurons\")\n",
    "plt.xlabel(\"Learning rate\")\n",
    "plt.title(\"Accuracy grid search for learning rate and number of neurons\")\n",
    "plt.savefig(\"accuracy_lr_nn_grid_search.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# different number of hidden layers\n",
    "\n",
    "n_epochs = 100\n",
    "eta = 0.01\n",
    "step = 5\n",
    "\n",
    "nn_1 = NeuralNetwork(30,class_cost_grad,random_state=1)\n",
    "nn_2 = NeuralNetwork(30,class_cost_grad,random_state=1)\n",
    "nn_3 = NeuralNetwork(30,class_cost_grad,random_state=1)\n",
    "layer11 = Layer(10,sigmoid,sigmoid_grad)\n",
    "layer21 = Layer(1,sigmoid,sigmoid_grad)\n",
    "layer12 = Layer(10,sigmoid,sigmoid_grad)\n",
    "layer22 = Layer(10,sigmoid,sigmoid_grad)\n",
    "layer32 = Layer(1,sigmoid,sigmoid_grad)\n",
    "layer13 = Layer(10,sigmoid,sigmoid_grad)\n",
    "layer23 = Layer(10,sigmoid,sigmoid_grad)\n",
    "layer33 = Layer(10,sigmoid,sigmoid_grad)\n",
    "layer43 = Layer(1,sigmoid,sigmoid_grad)\n",
    "\n",
    "nn_1.add_layer(layer11)\n",
    "nn_1.add_layer(layer21)\n",
    "nn_2.add_layer(layer12)\n",
    "nn_2.add_layer(layer22)\n",
    "nn_2.add_layer(layer32)\n",
    "nn_3.add_layer(layer13)\n",
    "nn_3.add_layer(layer23)\n",
    "nn_3.add_layer(layer33)\n",
    "nn_3.add_layer(layer43)\n",
    "\n",
    "acc_1 = []\n",
    "acc_2 = []\n",
    "acc_3 = []\n",
    "\n",
    "\n",
    "for i in range(0,n_epochs,step):\n",
    "    \n",
    "    acc_1.append(np.sum(nn_1.feed_forward_out(X_test).round() == y_test)/y_test.shape[0])\n",
    "    acc_2.append(np.sum(nn_2.feed_forward_out(X_test).round() == y_test)/y_test.shape[0])\n",
    "    acc_3.append(np.sum(nn_3.feed_forward_out(X_test).round() == y_test)/y_test.shape[0])\n",
    "    nn_1.train(X_train,y_train,eta=eta,epochs=step, minibatch_size = 20)\n",
    "    nn_2.train(X_train,y_train,eta=eta,epochs=step, minibatch_size = 20)\n",
    "    nn_3.train(X_train,y_train,eta=eta,epochs=step, minibatch_size = 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,8))\n",
    "plt.plot(range(0,n_epochs,step),acc_1,label='1 hidden layer')\n",
    "plt.plot(range(0,n_epochs,step),acc_2,label='2 hidden layers')\n",
    "plt.plot(range(0,n_epochs,step),acc_3,label='3 hidden layers')\n",
    "plt.legend()\n",
    "plt.title(\"Accuracy vs Epochs\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "#plt.savefig(\"class_number_of_hidden_layers.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# different activation functions\n",
    "\n",
    "n_epochs = 200\n",
    "step = 5\n",
    "eta = 0.01\n",
    "\n",
    "nn_sigm = NeuralNetwork(30, class_cost_grad,random_state=1)\n",
    "nn_relu = NeuralNetwork(30, class_cost_grad,random_state=1)\n",
    "nn_lrelu = NeuralNetwork(30, class_cost_grad,random_state=1)\n",
    "nn_tanh = NeuralNetwork(30, class_cost_grad,random_state=1)\n",
    "layer1sigm = Layer(10,sigmoid,sigmoid_grad)\n",
    "layer2sigm= Layer(1,sigmoid,sigmoid_grad)\n",
    "layer1relu = Layer(10,ReLU,ReLU_grad)\n",
    "layer2relu = Layer(1,sigmoid,sigmoid_grad)\n",
    "layer1lrelu = Layer(10,leakyReLU,leakyReLU_grad)\n",
    "layer2lrelu = Layer(1,sigmoid,sigmoid_grad)\n",
    "layer1tanh = Layer(10,tanh,tanh_grad)\n",
    "layer2tanh = Layer(1,sigmoid,sigmoid_grad)\n",
    "nn_sigm.add_layer(layer1sigm)\n",
    "nn_sigm.add_layer(layer2sigm)\n",
    "nn_relu.add_layer(layer1relu)\n",
    "nn_relu.add_layer(layer2relu)\n",
    "nn_lrelu.add_layer(layer1lrelu)\n",
    "nn_lrelu.add_layer(layer2lrelu)\n",
    "nn_tanh.add_layer(layer1tanh)\n",
    "nn_tanh.add_layer(layer2tanh)\n",
    "\n",
    "acc_sigm = []\n",
    "acc_relu = []\n",
    "acc_lrelu = []\n",
    "acc_tanh = []\n",
    "\n",
    "for i in range(0,n_epochs,step):\n",
    "    acc_sigm.append(np.sum(nn_sigm.feed_forward_out(X_test).round() == y_test)/y_test.shape[0])\n",
    "    acc_relu.append(np.sum(nn_relu.feed_forward_out(X_test).round() == y_test)/y_test.shape[0])\n",
    "    acc_lrelu.append(np.sum(nn_lrelu.feed_forward_out(X_test).round() == y_test)/y_test.shape[0])\n",
    "    acc_tanh.append(np.sum(nn_tanh.feed_forward_out(X_test).round() == y_test)/y_test.shape[0])\n",
    "    nn_sigm.train(X_train,y_train,eta=eta,epochs=step)\n",
    "    nn_relu.train(X_train,y_train,eta=eta,epochs=step)\n",
    "    nn_lrelu.train(X_train,y_train,eta=eta,epochs=step)\n",
    "    nn_tanh.train(X_train,y_train,eta=eta,epochs=step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,8))\n",
    "plt.plot(range(0,n_epochs,step),acc_sigm,label='Sigmoid')\n",
    "plt.plot(range(0,n_epochs,step),acc_relu,label='ReLU')\n",
    "plt.plot(range(0,n_epochs,step),acc_lrelu,label='Leaky ReLU')\n",
    "plt.plot(range(0,n_epochs,step),acc_tanh,label='Tanh')\n",
    "plt.legend()\n",
    "plt.title(\"Test accuracy vs Epochs for different activation functions\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "#plt.savefig(\"class_nn_activation_functions.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# grid search best parameters logistic regression\n",
    "\n",
    "# tuning learning rate and regularization parameter\n",
    "\n",
    "eta_list = [1e-4, 0.001, 0.01, 0.02, 0.05]\n",
    "reg_list = [0, 0.01, 0.1, 1, 10]\n",
    "accuracy = np.zeros((len(reg_list),len(eta_list)))\n",
    "\n",
    "for j in range(len(eta_list)):\n",
    "    for i in range(len(reg_list)):\n",
    "       \n",
    "        reg = reg_list[i]\n",
    "        eta = eta_list[j]\n",
    "\n",
    "        betas = logistic_regression_sgd(X_train, y_train_, eta, reg, 200, 50)\n",
    "        ypred = sigmoid(X_test @ betas)>0.5\n",
    "        accuracy[i][j] = sum(y_test.reshape((len(y_test_),)) == ypred)/np.shape(y_test_)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15,10))\n",
    "sns.heatmap(accuracy, xticklabels=eta_list, yticklabels=reg_list, annot=True, annot_kws={\"size\": 8.5}, fmt=\".3f\")\n",
    "plt.ylabel(\"Regularization parameter\")\n",
    "plt.xlabel(\"Learning rate\")\n",
    "plt.title(\"Accuracy grid search for learning rate and regularization parameter for logistic regression\")\n",
    "#plt.savefig(\"logreg_accuracy_lr_nn_grid_search.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# comparison with MLPClassifier and logistic regression\n",
    "\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "# Neural network\n",
    "nn = NeuralNetwork(30, class_cost_grad, random_state=1)\n",
    "l1 = Layer(30, sigmoid, sigmoid_grad)\n",
    "l2 = Layer(1, sigmoid, sigmoid_grad)\n",
    "nn.add_layer(l1)\n",
    "nn.add_layer(l2)\n",
    "\n",
    "nn.train(X_train,y_train, eta=0.01, regularization=0.1, epochs=200)\n",
    "\n",
    "pred = nn.feed_forward_out(X_test)\n",
    "pred = pred.round()\n",
    "nn_accuracy = np.sum(pred == y_test)/y_test.shape[0]\n",
    "\n",
    "\n",
    "# Logistic regression\n",
    "betas = logistic_regression_sgd(X_train, y_train_, 0.01, 0.1, 500, 50)\n",
    "ypred = sigmoid(X_test @ betas)>0.5\n",
    "logreg_accuracy = sum(y_test.reshape((len(y_test_),)) == ypred)/np.shape(y_test_)[0]\n",
    "\n",
    "# MLPClassifier\n",
    "dnn = MLPClassifier(hidden_layer_sizes=(50,), activation='logistic', learning_rate_init=0.01, max_iter=500)\n",
    "dnn.fit(X_train,y_train_)\n",
    "pred = dnn.predict(X_test)\n",
    "mlp_accuracy = np.sum(pred == y_test_)/y_test_.shape[0]\n",
    "\n",
    "print(\"Neural network accuracy: \" + str(nn_accuracy))\n",
    "print(\"Logistic regression accuracy: \" + str(logreg_accuracy))\n",
    "print(\"MLPClassifier accuracy: \" + str(mlp_accuracy))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.7 ('uni')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "418c59836fa3b9ce9db29957e801ce461de12293cee0e24ddf0e9f7bd846be9f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
