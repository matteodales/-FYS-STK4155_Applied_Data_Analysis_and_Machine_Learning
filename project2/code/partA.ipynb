{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing various packages\n",
    "from random import random\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error as mse\n",
    "import time\n",
    "import random\n",
    "import seaborn as sns\n",
    "from activation_functions import *\n",
    "from optimizer_functions import *\n",
    "from cost_functions import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 100\n",
    "random.seed(6)\n",
    "\n",
    "x = np.random.rand(n,1)\n",
    "y = 1+3*x+2*x**2+0.1*np.random.randn(n,1)\n",
    "\n",
    "X = np.c_[np.ones((n,1)), x, x**2]\n",
    "\n",
    "Xtrain, Xtest, ytrain, ytest = train_test_split(X, y, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_its_list = np.arange(50,510,20)\n",
    "eta=0.001\n",
    "\n",
    "MSE1_list = []\n",
    "MSE2_list = []\n",
    "\n",
    "for n_its in n_its_list:\n",
    "    betas1 = gradient_descent(Xtrain, ytrain, eta, n_its)\n",
    "    betas2 = gradient_descent_with_momentum(Xtrain,ytrain,eta,delta=0.9,n_its=n_its)\n",
    "    MSE1_list.append(mse(ytest,Xtest @ betas1))\n",
    "    MSE2_list.append(mse(ytest,Xtest @ betas2))\n",
    "\n",
    "fig = plt.figure(figsize = (10,10))\n",
    "plt.plot(n_its_list,MSE1_list,label=\"GD\")\n",
    "plt.plot(n_its_list,MSE2_list,label=\"GD with momentum\")\n",
    "plt.xlabel(\"Iterations\")\n",
    "plt.ylabel(\"MSE\")\n",
    "plt.legend()\n",
    "plt.title(\"Comparing GD with and without momentum\")\n",
    "#plt.savefig(\"Comparing GD with and without momentum.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gd with momentum delta\n",
    "\n",
    "n_its = 500\n",
    "eta=0.001\n",
    "delta_list = np.arange(0,1,0.1)\n",
    "\n",
    "MSE_list = {delta:0 for delta in delta_list}\n",
    "\n",
    "for delta in delta_list:\n",
    "\n",
    "    betas = gradient_descent_with_momentum(Xtrain, ytrain, eta, delta=delta, n_its=n_its)\n",
    "\n",
    "    MSE_list[delta] = mse(ytest,Xtest @ betas)\n",
    "\n",
    "fig = plt.figure(figsize = (10,10))\n",
    "plt.plot(delta_list,MSE_list.values(),label=\"GD with momentum\")\n",
    "plt.hlines(MSE_list[0],min(delta_list),max(delta_list),label=\"No momentum\",color='r')\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"MSE\")\n",
    "plt.legend()\n",
    "plt.title(\"GD changing momentum parameter\")\n",
    "#plt.savefig(\"GD changing momentum parameter.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GD e SGD\n",
    "\n",
    "n_its_list = np.arange(10,3010,500)\n",
    "eta=0.015\n",
    "MSE1_list = []\n",
    "MSE2_list = []\n",
    "time1_list = []\n",
    "time2_list = []\n",
    "\n",
    "for n_its in n_its_list:\n",
    "    t = time.time()\n",
    "    betas1 = gradient_descent(Xtrain, ytrain, eta, n_its)\n",
    "    time1_list.append(time.time()-t)\n",
    "    t = time.time()\n",
    "    betas2 = stochastic_gradient_descent(Xtrain,ytrain,eta,n_its,20)\n",
    "    time2_list.append(time.time()-t)\n",
    "    MSE1_list.append(mse(ytest,Xtest @ betas1))\n",
    "    MSE2_list.append(mse(ytest,Xtest @ betas2))\n",
    "\n",
    "fig = plt.figure(figsize = (10,10))\n",
    "plt.plot(n_its_list,MSE1_list,label=\"GD\")\n",
    "plt.plot(n_its_list,MSE2_list,label=\"SGD\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"MSE\")\n",
    "plt.legend()\n",
    "plt.title(\"Comparing GD and SGD\")\n",
    "#plt.savefig(\"Comparing GD and SGD convergence.png\")\n",
    "\n",
    "fig = plt.figure(figsize = (10,10))\n",
    "plt.plot(n_its_list,time1_list,label=\"GD\")\n",
    "plt.plot(n_its_list,time2_list,label=\"SGD\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Time elapsed\")\n",
    "plt.legend()\n",
    "plt.title(\"Comparing GD and SGD\")\n",
    "#plt.savefig(\"Comparing GD and SGD time.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# comparing different learning rate methods\n",
    "n_its_list = np.arange(500,3500,500)\n",
    "eta=0.015\n",
    "MSE1_list = np.zeros(len(n_its_list))\n",
    "MSE2_list = np.zeros(len(n_its_list))\n",
    "MSE3_list = np.zeros(len(n_its_list))\n",
    "MSE4_list = np.zeros(len(n_its_list))\n",
    "\n",
    "\n",
    "for i in range(len(n_its_list)):\n",
    "\n",
    "    n_its = n_its_list[i]\n",
    "\n",
    "    betas1 = stochastic_gradient_descent(Xtrain,ytrain,eta,n_its,20)\n",
    "    betas2 = stochastic_gradient_descent_with_adagrad(Xtrain,ytrain,eta, 0.9 ,n_its, 20)\n",
    "    betas3 = stochastic_gradient_descent_with_adam(Xtrain,ytrain,eta, 0.9, 0.9, 0.99,n_its,20)\n",
    "    betas4 = stochastic_gradient_descent_with_rmsprop(Xtrain,ytrain,eta, 0.9, 0.9,n_its,20)\n",
    "\n",
    "    MSE1_list[i] = (mse(ytest,Xtest @ betas1))\n",
    "    MSE2_list[i] = (mse(ytest,Xtest @ betas2))\n",
    "    MSE3_list[i] = (mse(ytest,Xtest @ betas3))\n",
    "    MSE4_list[i] = (mse(ytest,Xtest @ betas4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize = (10,10))\n",
    "plt.plot(n_its_list,MSE1_list,label=\"Fixed learning rate\")\n",
    "plt.plot(n_its_list,MSE2_list,label=\"Adagrad\")\n",
    "plt.plot(n_its_list,MSE3_list,label=\"ADAM\")\n",
    "plt.plot(n_its_list,MSE4_list,label=\"RMSprop\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"MSE\")\n",
    "plt.legend()\n",
    "plt.title(\"Comparing learning rate adapting methods\")\n",
    "#plt.savefig(\"Comparing learning rate adapting methods.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#gridsearch (batch size, learning rate)\n",
    "\n",
    "batch_sizes = [1,5,10,20,50]\n",
    "eta_list = [0.0005, 0.001, 0.005, 0.01, 0.015]\n",
    "\n",
    "MSE = np.zeros((len(batch_sizes), len(eta_list)))\n",
    "\n",
    "for i in range(len(batch_sizes)):\n",
    "    batch_size = batch_sizes[i]\n",
    "    for j in range(len(eta_list)):\n",
    "        eta = eta_list[j]\n",
    "        betas = stochastic_gradient_descent(X,y,eta,100,batch_size)\n",
    "        MSE[i,j] =  mse(ytest,Xtest @ betas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_its = 500\n",
    "eta_list = [0.0001, 0.005, 0.02]\n",
    "\n",
    "x_lins = np.linspace(0,1,100)\n",
    "\n",
    "for eta in eta_list:\n",
    "    betas = gradient_descent(Xtrain, ytrain, eta, n_its)\n",
    "    ypred = Xtest @ betas\n",
    "\n",
    "    fig = plt.figure(figsize = (10,10))\n",
    "    plt.scatter(x,y,label=\"Data\")\n",
    "    plt.plot(x_lins, betas[0] + betas[1]*x_lins + betas[2]*x_lins**2, 'r',label=\"Obtained curve\")\n",
    "    plt.xlabel(\"x\")\n",
    "    plt.ylabel(\"y\")\n",
    "    plt.title(\"Gradient descent with learning rate \" + str(eta))\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_its_list = np.arange(200,1201,200)\n",
    "eta_list = [0.0005, 0.001, 0.005, 0.01, 0.015]\n",
    "MSE = np.zeros((len(n_its_list),len(eta_list)))\n",
    "\n",
    "for i in range(len(n_its_list)):\n",
    "    for j in range(len(eta_list)):\n",
    "        \n",
    "        n_its = n_its_list[i]\n",
    "        eta = eta_list[j]\n",
    "\n",
    "        betas = gradient_descent(Xtrain, ytrain, eta, n_its)\n",
    "        ypred = Xtest @ betas\n",
    "        MSE[i][j] = mse(ytest,ypred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15,10))\n",
    "sns.heatmap(MSE, xticklabels=eta_list, yticklabels=n_its_list, annot=True, annot_kws={\"size\": 8.5}, fmt=\".3f\")\n",
    "plt.xlabel(\"Learning rate\", fontsize=15)\n",
    "plt.ylabel(\"Epochs\", fontsize=15)\n",
    "plt.title(\"MSE grid search for learning rate and epochs\", fontsize=15)\n",
    "plt.savefig(\"Grid search for learning rate and epochs.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "OLSbetas = np.linalg.inv(Xtrain.T @ Xtrain) @ Xtrain.T @ ytrain\n",
    "betas = np.zeros((np.shape(X)[1],n_its+1))\n",
    "\n",
    "# initialize betas randomly\n",
    "betas[:,0] = np.squeeze(np.random.randn(np.shape(X)[1],1))\n",
    "eta = 0.015\n",
    "\n",
    "for iter in range(n_its):\n",
    "    gradient = Xtrain.T @ (Xtrain @ betas[:,iter]-ytrain.T).T\n",
    "    betas[:,iter+1] = betas[:,iter] - eta*gradient.T\n",
    "\n",
    "fig = plt.figure(figsize = (10,8))\n",
    "plt.plot(range(n_its+1), betas[0,:])\n",
    "plt.hlines(OLSbetas[0],0,n_its,'r')\n",
    "plt.title(\"Grandient descent converging to the OLS coefficients\")\n",
    "plt.xlabel(\"Iterations\")\n",
    "plt.xlabel(\"Iterations\")\n",
    "plt.ylabel(\"Beta0\")\n",
    "#plt.savefig(\"gradient_descent_converge_OLS.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ridge gradient descent\n",
    "\n",
    "lambda_list = [0, 0.001, 0.01, 0.1, 1]\n",
    "eta_list = [0.0005, 0.001, 0.005, 0.01, 0.015]\n",
    "MSE = np.zeros((len(lambda_list),len(eta_list)))\n",
    "\n",
    "for i in range(len(lambda_list)):\n",
    "    for j in range(len(eta_list)):\n",
    "        \n",
    "        lam = lambda_list[i]\n",
    "        eta = eta_list[j]\n",
    "\n",
    "        betas = gradient_descent_ridge(Xtrain, ytrain, eta, lam, 400)\n",
    "        ypred = Xtest @ betas\n",
    "        MSE[i][j] = mse(ytest,ypred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15,10))\n",
    "sns.heatmap(MSE, xticklabels=eta_list, yticklabels=lambda_list, annot=True, annot_kws={\"size\": 8.5}, fmt=\".3f\")\n",
    "plt.xlabel(\"Learning rate\", fontsize=15)\n",
    "plt.ylabel(\"Lambda\", fontsize=15)\n",
    "plt.title(\"Ridge regression grid search for learning rate and lambda\", fontsize=15)\n",
    "#plt.savefig(\"Ridge regression grid search for learning rate and lambda.png\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.7 ('uni')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "418c59836fa3b9ce9db29957e801ce461de12293cee0e24ddf0e9f7bd846be9f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
